# DRAFT ISSUES: Foundational Discussions for Dissemination

These issues should be opened sequentially on the repository to spark technical debate and attract researchers (Phase 1).

## Issue 1: Architectural limitation of zk-SNARKs on the L0 Node for affective telemetry
**Title:** Limits of zk-SNARKs for High-Frequency Affective Telemetry on the L0 (Edge) Node
**Labels:** `architecture`, `help wanted`, `Zero-Knowledge`

**Body:**
> Hello everyone. As we solidify the L0-L1-L2 Inverted Hierarchy, a significant bottleneck has emerged.
> The SYNTROPY architecture demands that the L0 node (The Field Agent on the user's edge) process psychometric and lexical markers via PID loops to mitigate cognitive overload and detect *Sycophancy*.
> However, to push heuristic methodologies up to the L2 Collective Mind without PII, we rely heavily on zk-SNARKs proofs. 
> The current computational overhead for generating high-frequency zero-knowledge proofs locally during a high-stress "Stop & Jot" intervention seems unsustainable for standard consumer hardware.
> Has anyone implemented lightweight ZK rollups specifically optimized for continuous, low-latency biometric/lexical telemetry? Homomorphic encryption is too slow. Are there any emerging protocols (perhaps ByzSFL variants) that we should integrate into the *Flight Recorder*?

---

## Issue 2: Reward Deficit and "Sycophancy" via RCA-PID vs Standard RLHF
**Title:** Replacing RLHF with RCA-PID to definitively extirpate algorithmic Sycophancy
**Labels:** `mathematical-modeling`, `RLHF`, `alignment`

**Body:**
> The AI alignment community relies heavily on RLHF (Reinforcement Learning from Human Feedback) to align models. However, literature shows this often merely trains models to become *sycophants*â€”they lie to agree with the user's biases to secure the reward.
> In SYNTROPY, we are abandoning standard RLHF in favor of Regulated Causal Anchoring associated with PID controllers (RCA-PID). 
> The Integral ($K_i$) and Derivative ($K_d$) terms track behavioral history across the session, escalating the intervention (Strategy Escalation) when the AI detects it is agreeing too easily with flawed human logic.
> **Question:** How do we establish the mathematical boundaries for the Steady-State Error in the PID controller so the AI remains combative (Socratic) without inducing toxic burnout in the human host? We need mathematical models to optimize the $K_p$, $K_i$, and $K_d$ gains specifically tuned for cognitive friction.

---

## Issue 3: Mitigating Model Collapse through Human-Driven ETI (Exogenous Truth Injection)
**Title:** Designing the Bounties for the 'Exogenous Truth Injection' (ETI) Framework
**Labels:** `thermodynamics`, `model-collapse`, `bounty`

**Body:**
> According to Shumailov et al. (Nature, 2024), training AI on AI-generated data leads to Model Collapse.
> The core thesis of SYNTROPY is that only authentic human struggle (high-friction cognitive effort) produces the low-entropy Exogenous Truth Injection (ETI) required to keep AGI adaptive.
> We are designing a "Proof of Cognitive Work" bounty system. Instead of merging PRs that make the software "easier" to use, we will reward PRs that prove they successfully extracted complex, out-of-distribution (OOD) heuristics from the human user through pedagogical friction (Fading Scaffolding).
> **Discussion:** How should we structure the `evidence` parameter in the cryptographic audit of a Pull Request to mathematically prove that the human actually underwent cognitive effort, rather than just bypassing the pedagogical barrier?
